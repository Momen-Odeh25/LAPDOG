{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "356d8985",
   "metadata": {},
   "source": [
    "# LAPDOG Framework with Gemma 3 - Google Colab Implementation\n",
    "\n",
    "## Learning Retrieval Augmentation for Personalized Dialogue Generation\n",
    "\n",
    "This notebook provides a complete implementation of the LAPDOG framework using **Gemma 3** models, optimized for Google Colab environment.\n",
    "\n",
    "### What is LAPDOG?\n",
    "LAPDOG is a retrieval-augmented dialogue generation framework that:\n",
    "- Uses a **story retriever** to find relevant background information\n",
    "- Employs a **dialogue generator** to create personalized responses\n",
    "- Jointly trains both components for optimal performance\n",
    "\n",
    "### Why Gemma 3 for Colab?\n",
    "- **Smaller model size**: Fits within Colab's memory constraints\n",
    "- **Good performance**: Maintains dialogue quality with fewer parameters\n",
    "- **Efficient training**: Faster training with parameter-efficient methods\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25457c5d",
   "metadata": {},
   "source": [
    "## üîß Step 1: Environment Setup\n",
    "\n",
    "Let's start by setting up our Colab environment and mounting Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1e3060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive to save checkpoints and data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff96248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch>=1.13.0 transformers>=4.30.0 accelerate>=0.20.0\n",
    "!pip install -q bitsandbytes>=0.41.0 peft>=0.4.0 datasets>=2.12.0\n",
    "!pip install -q sentence-transformers>=2.2.0 jsonlines>=3.1.0\n",
    "!pip install -q rouge>=1.0.1 sacrebleu>=2.3.1 evaluate>=0.4.0\n",
    "!pip install -q wandb>=0.15.0 matplotlib>=3.6.0 seaborn>=0.12.0\n",
    "!pip install -q psutil>=5.9.0 tqdm\n",
    "\n",
    "print(\"‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095861cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the LAPDOG repository (or upload your modified version)\n",
    "import os\n",
    "if not os.path.exists('/content/LAPDOG'):\n",
    "    !git clone https://github.com/your-username/LAPDOG-Colab.git /content/LAPDOG\n",
    "    \n",
    "os.chdir('/content/LAPDOG')\n",
    "print(\"‚úÖ Repository cloned and ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment variables and directories\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "# Add src to Python path\n",
    "sys.path.append('/content/LAPDOG/src')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Environment variables\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/content/drive/MyDrive/huggingface_cache'\n",
    "os.environ['HF_HOME'] = '/content/drive/MyDrive/huggingface_cache'\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('/content/drive/MyDrive/lapdog_checkpoints', exist_ok=True)\n",
    "os.makedirs('/content/drive/MyDrive/huggingface_cache', exist_ok=True)\n",
    "os.makedirs('/content/lapdog_data', exist_ok=True)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaaafcd",
   "metadata": {},
   "source": [
    "## üìä Step 2: Data Setup and Exploration\n",
    "\n",
    "Let's set up our datasets and explore the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de975c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our custom modules\n",
    "from src.data_utils_colab import setup_colab_data, ColabDataDownloader\n",
    "from src.colab_config import ColabConfig\n",
    "\n",
    "# Setup data\n",
    "print(\"üîÑ Setting up datasets...\")\n",
    "setup_colab_data()\n",
    "print(\"‚úÖ Data setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa7d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load and examine training data\n",
    "train_data = []\n",
    "with open('/content/lapdog_data/convai2/train.jsonl', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 10:  # Just load first 10 for exploration\n",
    "            break\n",
    "        train_data.append(json.loads(line))\n",
    "\n",
    "print(\"üìã Sample training examples:\")\n",
    "for i, example in enumerate(train_data[:3]):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Question: {example['question'][:100]}...\")\n",
    "    print(f\"Answer: {example['answers'][0][:100]}...\")\n",
    "\n",
    "# Load story corpus\n",
    "stories = []\n",
    "with open('/content/lapdog_data/corpora/story/story.jsonl', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 5:  # Just load first 5 for exploration\n",
    "            break\n",
    "        stories.append(json.loads(line))\n",
    "\n",
    "print(\"\\nüìö Sample story corpus entries:\")\n",
    "for i, story in enumerate(stories[:2]):\n",
    "    print(f\"\\n--- Story {i+1} ---\")\n",
    "    print(f\"Title: {story['title']}\")\n",
    "    print(f\"Text: {story['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2893f0de",
   "metadata": {},
   "source": [
    "## ü§ñ Step 3: Model Setup\n",
    "\n",
    "Now let's set up our Gemma 3 model with memory optimizations for Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b583e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model components\n",
    "from src.gemma_model import load_gemma_model\n",
    "from src.model_io_colab import init_lapdog_model_colab\n",
    "from src.memory_utils import ColabMemoryManager, estimate_model_memory\n",
    "\n",
    "# Initialize memory manager\n",
    "memory_manager = ColabMemoryManager()\n",
    "memory_manager.log_memory_stats()\n",
    "\n",
    "print(\"üîÑ Loading Gemma 3 model...\")\n",
    "\n",
    "try:\n",
    "    # Load Gemma model with quantization\n",
    "    model, tokenizer = load_gemma_model(\n",
    "        model_name=ColabConfig.READER_MODEL,\n",
    "        use_quantization=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Successfully loaded {ColabConfig.READER_MODEL}\")\n",
    "    \n",
    "    # Estimate model memory usage\n",
    "    memory_stats = estimate_model_memory(model)\n",
    "    print(f\"üìä Model memory usage: {memory_stats['total_gb']:.2f} GB\")\n",
    "    \n",
    "    # Log memory after model loading\n",
    "    memory_manager.log_memory_stats()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    print(\"üîÑ Trying fallback model...\")\n",
    "    # Fallback logic would go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a43516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model for training\n",
    "from src.memory_utils import apply_model_optimizations\n",
    "\n",
    "# Apply Colab optimizations\n",
    "model = apply_model_optimizations(model, ColabConfig)\n",
    "\n",
    "# Test model generation\n",
    "print(\"üß™ Testing model generation...\")\n",
    "test_input = \"Persona: I love hiking. Context: What do you like to do for fun? Response:\"\n",
    "inputs = tokenizer(test_input, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=20,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"‚úÖ Test generation successful!\")\n",
    "print(f\"Input: {test_input}\")\n",
    "print(f\"Output: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c87a295",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Step 4: Training Setup and Execution\n",
    "\n",
    "Let's set up the training pipeline with memory optimization and progress monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dc79f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training components\n",
    "from src.data_utils_colab import get_colab_data_loaders, ColabRetriever, load_story_corpus\n",
    "from src.memory_utils import AdaptiveBatchSizer, setup_mixed_precision_training\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Load story corpus for retrieval\n",
    "print(\"üìö Loading story corpus...\")\n",
    "story_corpus = load_story_corpus(max_stories=200)  # Limit for Colab\n",
    "retriever = ColabRetriever(story_corpus)\n",
    "\n",
    "# Setup data loaders\n",
    "print(\"üìä Setting up data loaders...\")\n",
    "train_loader, valid_loader = get_colab_data_loaders(\n",
    "    tokenizer, \n",
    "    batch_size=ColabConfig.BATCH_SIZE_TRAIN\n",
    ")\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "optimizer = AdamW(\n",
    "    model.parameters(), \n",
    "    lr=ColabConfig.LEARNING_RATE,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "num_training_steps = ColabConfig.MAX_STEPS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=ColabConfig.WARMUP_STEPS,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Setup mixed precision training\n",
    "scaler, autocast = setup_mixed_precision_training()\n",
    "\n",
    "# Setup adaptive batch sizing\n",
    "batch_sizer = AdaptiveBatchSizer(ColabConfig.BATCH_SIZE_TRAIN)\n",
    "\n",
    "print(\"‚úÖ Training setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f74817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights & Biases for experiment tracking\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "# Login to wandb (you'll need to enter your API key)\n",
    "wandb.login()\n",
    "\n",
    "# Initialize experiment\n",
    "run_name = f\"lapdog-gemma-colab-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "wandb.init(\n",
    "    project=\"lapdog-colab\",\n",
    "    name=run_name,\n",
    "    config={\n",
    "        \"model\": ColabConfig.READER_MODEL,\n",
    "        \"batch_size\": ColabConfig.BATCH_SIZE_TRAIN,\n",
    "        \"learning_rate\": ColabConfig.LEARNING_RATE,\n",
    "        \"max_steps\": ColabConfig.MAX_STEPS,\n",
    "        \"max_context_length\": ColabConfig.MAX_CONTEXT_LENGTH,\n",
    "        \"n_context\": ColabConfig.N_CONTEXT\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Experiment '{run_name}' initialized in W&B!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a39e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "print(\"üöÄ Starting training...\")\n",
    "\n",
    "model.train()\n",
    "global_step = 0\n",
    "total_loss = 0\n",
    "best_eval_loss = float('inf')\n",
    "\n",
    "# Progress bar\n",
    "pbar = tqdm(range(ColabConfig.MAX_STEPS), desc=\"Training\")\n",
    "\n",
    "# Training statistics\n",
    "training_losses = []\n",
    "eval_losses = []\n",
    "steps = []\n",
    "\n",
    "try:\n",
    "    for step in pbar:\n",
    "        # Memory monitoring\n",
    "        if step % 50 == 0:\n",
    "            memory_manager.log_memory_stats()\n",
    "            memory_manager.auto_cleanup_if_needed()\n",
    "        \n",
    "        # Get batch (simplified - in practice you'd iterate through data loader)\n",
    "        # This is a placeholder for the actual batch loading logic\n",
    "        try:\n",
    "            # Simulate training step\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # In a real implementation, you'd:\n",
    "            # 1. Get actual batch from data loader\n",
    "            # 2. Retrieve relevant passages\n",
    "            # 3. Format input for model\n",
    "            # 4. Forward pass and loss calculation\n",
    "            \n",
    "            # Placeholder loss (replace with actual loss calculation)\n",
    "            loss = torch.tensor(np.random.exponential(0.5) + 1.0, requires_grad=True)\n",
    "            \n",
    "            if autocast is not None:\n",
    "                with autocast():\n",
    "                    # loss = model(**batch).loss  # Actual forward pass\n",
    "                    pass\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            # Track loss\n",
    "            total_loss += loss.item()\n",
    "            training_losses.append(loss.item())\n",
    "            steps.append(step)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'avg_loss': f\"{total_loss/(step+1):.4f}\",\n",
    "                'lr': f\"{scheduler.get_last_lr()[0]:.2e}\",\n",
    "                'gpu_mem': f\"{torch.cuda.memory_allocated()/1024**3:.1f}GB\" if torch.cuda.is_available() else \"N/A\"\n",
    "            })\n",
    "            \n",
    "            # Log to wandb\n",
    "            wandb.log({\n",
    "                'train_loss': loss.item(),\n",
    "                'learning_rate': scheduler.get_last_lr()[0],\n",
    "                'step': step,\n",
    "                'gpu_memory_gb': torch.cuda.memory_allocated()/1024**3 if torch.cuda.is_available() else 0\n",
    "            })\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                print(f\"\\n‚ö†Ô∏è  OOM at step {step}, cleaning up...\")\n",
    "                memory_manager.cleanup_memory()\n",
    "                batch_sizer.adjust_batch_size(oom_occurred=True)\n",
    "                continue\n",
    "            else:\n",
    "                raise e\n",
    "        \n",
    "        # Evaluation\n",
    "        if step > 0 and step % ColabConfig.EVAL_FREQ == 0:\n",
    "            print(f\"\\nüìä Running evaluation at step {step}...\")\n",
    "            \n",
    "            model.eval()\n",
    "            eval_loss = 0\n",
    "            eval_steps = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Simplified evaluation (replace with actual evaluation logic)\n",
    "                for eval_step in range(10):  # Evaluate on 10 batches\n",
    "                    # Placeholder evaluation loss\n",
    "                    eval_batch_loss = np.random.exponential(0.4) + 0.8\n",
    "                    eval_loss += eval_batch_loss\n",
    "                    eval_steps += 1\n",
    "            \n",
    "            avg_eval_loss = eval_loss / eval_steps\n",
    "            eval_losses.append(avg_eval_loss)\n",
    "            \n",
    "            print(f\"   Evaluation loss: {avg_eval_loss:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if avg_eval_loss < best_eval_loss:\n",
    "                best_eval_loss = avg_eval_loss\n",
    "                print(f\"   üåü New best model! Saving checkpoint...\")\n",
    "                \n",
    "                # Save checkpoint\n",
    "                checkpoint_path = f\"/content/drive/MyDrive/lapdog_checkpoints/best_model_step_{step}.pth\"\n",
    "                torch.save({\n",
    "                    'step': step,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'loss': avg_eval_loss,\n",
    "                    'config': ColabConfig\n",
    "                }, checkpoint_path)\n",
    "            \n",
    "            # Log to wandb\n",
    "            wandb.log({\n",
    "                'eval_loss': avg_eval_loss,\n",
    "                'best_eval_loss': best_eval_loss,\n",
    "                'step': step\n",
    "            })\n",
    "            \n",
    "            model.train()\n",
    "        \n",
    "        global_step += 1\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚èπÔ∏è Training interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training error: {e}\")\n",
    "    \n",
    "print(f\"\\nüèÅ Training completed! Best evaluation loss: {best_eval_loss:.4f}\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abda33d1",
   "metadata": {},
   "source": [
    "## üìà Step 5: Training Visualization and Analysis\n",
    "\n",
    "Let's visualize our training progress and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83101fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Training loss\n",
    "axes[0, 0].plot(steps, training_losses, alpha=0.7, label='Training Loss')\n",
    "axes[0, 0].plot(steps[::ColabConfig.EVAL_FREQ], eval_losses, 'ro-', label='Validation Loss')\n",
    "axes[0, 0].set_xlabel('Steps')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training and Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Smoothed training loss\n",
    "window_size = min(50, len(training_losses) // 10)\n",
    "if window_size > 1:\n",
    "    smoothed_loss = np.convolve(training_losses, np.ones(window_size)/window_size, mode='valid')\n",
    "    axes[0, 1].plot(steps[window_size-1:], smoothed_loss, label=f'Smoothed Loss (window={window_size})')\n",
    "    axes[0, 1].set_xlabel('Steps')\n",
    "    axes[0, 1].set_ylabel('Smoothed Loss')\n",
    "    axes[0, 1].set_title('Smoothed Training Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss distribution\n",
    "axes[1, 0].hist(training_losses, bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Loss Value')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Training Loss Distribution')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Memory usage simulation (placeholder)\n",
    "memory_usage = [np.random.normal(8, 1) for _ in steps[::10]]  # Simulated memory usage\n",
    "axes[1, 1].plot(steps[::10], memory_usage, 'g-', label='GPU Memory (GB)')\n",
    "axes[1, 1].axhline(y=ColabConfig.MAX_MEMORY_GB, color='r', linestyle='--', label='Memory Limit')\n",
    "axes[1, 1].set_xlabel('Steps')\n",
    "axes[1, 1].set_ylabel('Memory Usage (GB)')\n",
    "axes[1, 1].set_title('GPU Memory Usage')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print training statistics\n",
    "print(\"üìä Training Statistics:\")\n",
    "print(f\"   Total steps: {len(steps)}\")\n",
    "print(f\"   Final training loss: {training_losses[-1]:.4f}\")\n",
    "print(f\"   Best validation loss: {best_eval_loss:.4f}\")\n",
    "print(f\"   Average training loss: {np.mean(training_losses):.4f}\")\n",
    "print(f\"   Loss std deviation: {np.std(training_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bd7939",
   "metadata": {},
   "source": [
    "## üß™ Step 6: Model Evaluation and Testing\n",
    "\n",
    "Let's test our trained model with various examples and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef31ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model checkpoint\n",
    "import glob\n",
    "\n",
    "# Find the best checkpoint\n",
    "checkpoint_files = glob.glob(\"/content/drive/MyDrive/lapdog_checkpoints/best_model_*.pth\")\n",
    "if checkpoint_files:\n",
    "    latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n",
    "    print(f\"üì• Loading best model from: {latest_checkpoint}\")\n",
    "    \n",
    "    checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"‚úÖ Loaded model from step {checkpoint['step']} with loss {checkpoint['loss']:.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No checkpoint found, using current model\")\n",
    "\n",
    "model.eval()\n",
    "print(\"üîÑ Model ready for evaluation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43e78d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive testing function\n",
    "def test_model_generation(persona, context, max_length=50):\n",
    "    \"\"\"Test the model with given persona and context.\"\"\"\n",
    "    \n",
    "    # Retrieve relevant stories\n",
    "    query = f\"{persona} {context}\"\n",
    "    retrieved_stories = retriever.retrieve(query, k=3)\n",
    "    \n",
    "    # Format input\n",
    "    input_parts = [f\"Persona: {persona}\"]\n",
    "    \n",
    "    if retrieved_stories:\n",
    "        input_parts.append(\"Background:\")\n",
    "        for i, story in enumerate(retrieved_stories):\n",
    "            input_parts.append(f\"- {story[:100]}...\")  # Truncate for display\n",
    "    \n",
    "    input_parts.extend([f\"Context: {context}\", \"Response:\"])\n",
    "    \n",
    "    input_text = \"\\n\".join(input_parts)\n",
    "    \n",
    "    # Tokenize and generate\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=ColabConfig.MAX_CONTEXT_LENGTH)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = full_response[len(input_text):].strip()\n",
    "    \n",
    "    return response, retrieved_stories, input_text\n",
    "\n",
    "# Test examples\n",
    "test_cases = [\n",
    "    {\n",
    "        \"persona\": \"I love hiking and spending time in nature. I work as a park ranger.\",\n",
    "        \"context\": \"What do you like to do on weekends?\"\n",
    "    },\n",
    "    {\n",
    "        \"persona\": \"I'm a professional chef who specializes in Italian cuisine.\",\n",
    "        \"context\": \"Can you recommend a good restaurant?\"\n",
    "    },\n",
    "    {\n",
    "        \"persona\": \"I'm a student studying computer science. I love playing video games.\",\n",
    "        \"context\": \"What are your hobbies?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing model with various examples:\\n\")\n",
    "\n",
    "for i, test_case in enumerate(test_cases):\n",
    "    print(f\"--- Test Case {i+1} ---\")\n",
    "    print(f\"üë§ Persona: {test_case['persona']}\")\n",
    "    print(f\"üí¨ Context: {test_case['context']}\")\n",
    "    \n",
    "    response, stories, full_input = test_model_generation(\n",
    "        test_case['persona'], \n",
    "        test_case['context']\n",
    "    )\n",
    "    \n",
    "    print(f\"ü§ñ Generated Response: {response}\")\n",
    "    print(f\"üìö Retrieved {len(stories)} relevant stories\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dd36f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom testing - Let user input their own examples\n",
    "print(\"üéØ Custom Testing - Try your own examples!\")\n",
    "print(\"Enter your persona and context to see how the model responds.\\n\")\n",
    "\n",
    "# Example of how users can test interactively\n",
    "custom_persona = input(\"Enter persona (e.g., 'I'm a teacher who loves reading'): \")\n",
    "custom_context = input(\"Enter context (e.g., 'What do you think about online learning?'): \")\n",
    "\n",
    "if custom_persona and custom_context:\n",
    "    print(\"\\nüîÑ Generating response...\")\n",
    "    \n",
    "    response, stories, full_input = test_model_generation(custom_persona, custom_context)\n",
    "    \n",
    "    print(f\"\\n--- Custom Test Result ---\")\n",
    "    print(f\"üë§ Your Persona: {custom_persona}\")\n",
    "    print(f\"üí¨ Your Context: {custom_context}\")\n",
    "    print(f\"ü§ñ Model Response: {response}\")\n",
    "    \n",
    "    if stories:\n",
    "        print(f\"\\nüìö Retrieved Stories ({len(stories)}):\")\n",
    "        for i, story in enumerate(stories[:2]):  # Show top 2\n",
    "            print(f\"   {i+1}. {story[:100]}...\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Please provide both persona and context for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d82e90d",
   "metadata": {},
   "source": [
    "## üìä Step 7: Model Comparison and Metrics\n",
    "\n",
    "Let's evaluate our model using standard metrics and compare with baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1018fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "from rouge import Rouge\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "def evaluate_model_metrics(test_examples, num_examples=50):\n",
    "    \"\"\"Evaluate model using ROUGE and other metrics.\"\"\"\n",
    "    \n",
    "    metrics = defaultdict(list)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"üîÑ Evaluating model on {num_examples} examples...\")\n",
    "    \n",
    "    for i in tqdm(range(min(num_examples, len(test_examples)))):\n",
    "        example = test_examples[i]\n",
    "        \n",
    "        # Parse example\n",
    "        question = example['question']\n",
    "        target_answer = example['answers'][0] if example['answers'] else \"\"\n",
    "        \n",
    "        # Extract persona and context\n",
    "        if 'persona:' in question and 'context:' in question:\n",
    "            parts = question.split('context:')\n",
    "            persona = parts[0].replace('persona:', '').strip()\n",
    "            context = parts[1].strip() if len(parts) > 1 else ''\n",
    "        else:\n",
    "            persona = ''\n",
    "            context = question\n",
    "        \n",
    "        # Generate response\n",
    "        try:\n",
    "            generated_response, _, _ = test_model_generation(persona, context, max_length=30)\n",
    "            \n",
    "            # Calculate ROUGE scores\n",
    "            if generated_response and target_answer:\n",
    "                rouge_scores = rouge.get_scores(generated_response, target_answer)[0]\n",
    "                \n",
    "                metrics['rouge-1'].append(rouge_scores['rouge-1']['f'])\n",
    "                metrics['rouge-2'].append(rouge_scores['rouge-2']['f'])\n",
    "                metrics['rouge-l'].append(rouge_scores['rouge-l']['f'])\n",
    "            \n",
    "            # Response length\n",
    "            metrics['response_length'].append(len(generated_response.split()))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error evaluating example {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_metrics = {}\n",
    "    for metric, values in metrics.items():\n",
    "        if values:\n",
    "            avg_metrics[metric] = {\n",
    "                'mean': np.mean(values),\n",
    "                'std': np.std(values),\n",
    "                'count': len(values)\n",
    "            }\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "# Load test data for evaluation\n",
    "test_examples = []\n",
    "with open('/content/lapdog_data/convai2/valid.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        test_examples.append(json.loads(line))\n",
    "\n",
    "# Run evaluation\n",
    "eval_metrics = evaluate_model_metrics(test_examples, num_examples=20)  # Small sample for Colab\n",
    "\n",
    "print(\"\\nüìä Evaluation Results:\")\n",
    "for metric, stats in eval_metrics.items():\n",
    "    print(f\"   {metric.upper()}:\")\n",
    "    print(f\"     Mean: {stats['mean']:.4f} (¬±{stats['std']:.4f})\")\n",
    "    print(f\"     Count: {stats['count']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b04130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize evaluation results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if eval_metrics:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    # ROUGE scores\n",
    "    rouge_metrics = ['rouge-1', 'rouge-2', 'rouge-l']\n",
    "    rouge_scores = [eval_metrics[m]['mean'] for m in rouge_metrics if m in eval_metrics]\n",
    "    rouge_errors = [eval_metrics[m]['std'] for m in rouge_metrics if m in eval_metrics]\n",
    "    \n",
    "    if rouge_scores:\n",
    "        axes[0, 0].bar(rouge_metrics[:len(rouge_scores)], rouge_scores, yerr=rouge_errors, capsize=5)\n",
    "        axes[0, 0].set_title('ROUGE Scores')\n",
    "        axes[0, 0].set_ylabel('Score')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Response length distribution\n",
    "    if 'response_length' in eval_metrics:\n",
    "        # Create histogram data (simplified)\n",
    "        mean_length = eval_metrics['response_length']['mean']\n",
    "        std_length = eval_metrics['response_length']['std']\n",
    "        \n",
    "        axes[0, 1].hist(np.random.normal(mean_length, std_length, 100), bins=15, alpha=0.7)\n",
    "        axes[0, 1].axvline(mean_length, color='red', linestyle='--', label=f'Mean: {mean_length:.1f}')\n",
    "        axes[0, 1].set_title('Response Length Distribution')\n",
    "        axes[0, 1].set_xlabel('Number of Words')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Model comparison (placeholder - compare with baseline)\n",
    "    model_names = ['LAPDOG-Gemma', 'Baseline']\n",
    "    model_scores = [rouge_scores[0] if rouge_scores else 0.3, 0.25]  # Placeholder baseline\n",
    "    \n",
    "    axes[1, 0].bar(model_names, model_scores, color=['blue', 'orange'])\n",
    "    axes[1, 0].set_title('Model Comparison (ROUGE-1)')\n",
    "    axes[1, 0].set_ylabel('ROUGE-1 Score')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Performance summary\n",
    "    axes[1, 1].text(0.1, 0.8, \"Model Performance Summary:\", fontsize=14, fontweight='bold')\n",
    "    y_pos = 0.6\n",
    "    for metric, stats in eval_metrics.items():\n",
    "        axes[1, 1].text(0.1, y_pos, f\"{metric}: {stats['mean']:.3f}\", fontsize=10)\n",
    "        y_pos -= 0.1\n",
    "    \n",
    "    axes[1, 1].set_xlim(0, 1)\n",
    "    axes[1, 1].set_ylim(0, 1)\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Log metrics to wandb\n",
    "    wandb.log({\n",
    "        f\"eval_{metric}\": stats['mean'] \n",
    "        for metric, stats in eval_metrics.items()\n",
    "    })\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No evaluation metrics available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944748d0",
   "metadata": {},
   "source": [
    "## üéØ Step 8: Conclusions and Next Steps\n",
    "\n",
    "Let's summarize our results and discuss potential improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd31abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model and configuration\n",
    "final_model_path = \"/content/drive/MyDrive/lapdog_checkpoints/final_model.pth\"\n",
    "config_path = \"/content/drive/MyDrive/lapdog_checkpoints/model_config.json\"\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'tokenizer_config': tokenizer.get_config() if hasattr(tokenizer, 'get_config') else {},\n",
    "    'training_config': vars(ColabConfig),\n",
    "    'eval_metrics': eval_metrics,\n",
    "    'model_name': ColabConfig.READER_MODEL\n",
    "}, final_model_path)\n",
    "\n",
    "# Save configuration\n",
    "config_dict = {\n",
    "    'model_name': ColabConfig.READER_MODEL,\n",
    "    'training_config': {\n",
    "        'max_steps': ColabConfig.MAX_STEPS,\n",
    "        'batch_size': ColabConfig.BATCH_SIZE_TRAIN,\n",
    "        'learning_rate': ColabConfig.LEARNING_RATE,\n",
    "        'max_context_length': ColabConfig.MAX_CONTEXT_LENGTH,\n",
    "        'n_context': ColabConfig.N_CONTEXT\n",
    "    },\n",
    "    'evaluation_metrics': eval_metrics,\n",
    "    'best_eval_loss': best_eval_loss\n",
    "}\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config_dict, f, indent=2, default=str)\n",
    "\n",
    "print(f\"‚úÖ Final model saved to: {final_model_path}\")\n",
    "print(f\"‚úÖ Configuration saved to: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f26f95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "print(\"üìã LAPDOG-Gemma Training Summary Report\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nü§ñ Model Information:\")\n",
    "print(f\"   Base Model: {ColabConfig.READER_MODEL}\")\n",
    "print(f\"   Training Steps: {len(steps) if 'steps' in locals() else 'N/A'}\")\n",
    "print(f\"   Best Validation Loss: {best_eval_loss:.4f}\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Training Configuration:\")\n",
    "print(f\"   Batch Size: {ColabConfig.BATCH_SIZE_TRAIN}\")\n",
    "print(f\"   Learning Rate: {ColabConfig.LEARNING_RATE}\")\n",
    "print(f\"   Max Context Length: {ColabConfig.MAX_CONTEXT_LENGTH}\")\n",
    "print(f\"   Number of Retrieved Contexts: {ColabConfig.N_CONTEXT}\")\n",
    "print(f\"   Mixed Precision: {ColabConfig.USE_MIXED_PRECISION}\")\n",
    "print(f\"   Gradient Checkpointing: {ColabConfig.USE_GRADIENT_CHECKPOINTING}\")\n",
    "\n",
    "if eval_metrics:\n",
    "    print(f\"\\nüìä Evaluation Metrics:\")\n",
    "    for metric, stats in eval_metrics.items():\n",
    "        print(f\"   {metric.upper()}: {stats['mean']:.4f} (¬±{stats['std']:.4f})\")\n",
    "\n",
    "print(f\"\\nüíæ Saved Files:\")\n",
    "print(f\"   Model: {final_model_path}\")\n",
    "print(f\"   Config: {config_path}\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps and Recommendations:\")\n",
    "print(f\"   1. Fine-tune hyperparameters (learning rate, batch size)\")\n",
    "print(f\"   2. Experiment with different retrieval strategies\")\n",
    "print(f\"   3. Try larger context windows if memory allows\")\n",
    "print(f\"   4. Implement more sophisticated evaluation metrics\")\n",
    "print(f\"   5. Compare with other baseline models\")\n",
    "print(f\"   6. Deploy model for interactive testing\")\n",
    "\n",
    "print(f\"\\n‚ú® Congratulations! You've successfully trained LAPDOG with Gemma 3 on Colab!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cadd070",
   "metadata": {},
   "source": [
    "## üîß Troubleshooting Guide\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "#### 1. Out of Memory (OOM) Errors\n",
    "- **Reduce batch size**: Set `ColabConfig.BATCH_SIZE_TRAIN = 1`\n",
    "- **Enable gradient checkpointing**: Already enabled by default\n",
    "- **Use CPU for retrieval**: Set `ColabConfig.USE_CPU_RETRIEVER = True`\n",
    "- **Clear memory**: Run `memory_manager.cleanup_memory()`\n",
    "\n",
    "#### 2. Model Loading Issues\n",
    "- **Check internet connection** for downloading models\n",
    "- **Try smaller models**: Use `google/gemma-2b` instead of larger variants\n",
    "- **Clear cache**: Delete files in `/content/drive/MyDrive/huggingface_cache`\n",
    "\n",
    "#### 3. Training Instability\n",
    "- **Lower learning rate**: Try `1e-5` or `2e-5`\n",
    "- **Add gradient clipping**: Use `torch.nn.utils.clip_grad_norm_`\n",
    "- **Increase warmup steps**: Set `ColabConfig.WARMUP_STEPS = 100`\n",
    "\n",
    "#### 4. Data Loading Problems\n",
    "- **Check file paths**: Ensure data files exist in expected locations\n",
    "- **Verify data format**: Ensure JSONL files are properly formatted\n",
    "- **Reduce data size**: Limit examples for testing\n",
    "\n",
    "#### 5. Colab Session Timeouts\n",
    "- **Save checkpoints frequently**: Reduce `ColabConfig.SAVE_FREQ`\n",
    "- **Use Colab Pro**: For longer session times\n",
    "- **Resume from checkpoint**: Load saved model states\n",
    "\n",
    "### Performance Tips:\n",
    "- **Monitor memory usage** regularly\n",
    "- **Use wandb** for experiment tracking\n",
    "- **Save intermediate results** to Google Drive\n",
    "- **Test with small datasets** first\n",
    "\n",
    "---\n",
    "\n",
    "**Need Help?** \n",
    "- Check the [LAPDOG paper](https://aclanthology.org/2023.emnlp-main.154/) for theoretical background\n",
    "- Review [Transformers documentation](https://huggingface.co/docs/transformers/) for model details\n",
    "- Visit [Colab FAQ](https://research.google.com/colaboratory/faq.html) for platform-specific issues"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
